{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Trending Articles Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web Scrapping PubMed Trending Pages \n",
    "\n",
    "all_contents = []\n",
    "id_pattern = re.compile(r'data-article-id=\"\\d*\"')\n",
    "pmid_pattern = re.compile(r'\\n?PMID:\\s*\\d{8}')\n",
    "year_pattern = re.compile(r'\\.\\s+\\d{4}\\.\\n')\n",
    "name_pattern = re.compile(r'^\\n*.*\\n?\\.?\\n')\n",
    "jour_pattern = re.compile(r'\\n.*\\.\\s*\\d{4}\\.\\n*PMID:')\n",
    "access_pattern = re.compile(r'Free\\s*article\\.|Free\\s*PMC\\s*article\\.', re.I)\n",
    "pub_pattern = re.compile(r'\\D\\n.*\\D$')\n",
    "\n",
    "for page in range(1, 6):\n",
    "    # Web scrapping pubMed Trending Pages\n",
    "    r = requests.get(f\"https://pubmed.ncbi.nlm.nih.gov/trending/?size=200&page={page}\", allow_redirects=True)\n",
    "    try:\n",
    "        r.raise_for_status()\n",
    "    except Exception as exc:\n",
    "        print('There was a problem: %s' % (exc))\n",
    "\n",
    "    # Creating an PubMed Trending html file.\n",
    "    with open(f\"PubMed Trending Page{page}.html\", 'wb') as file:\n",
    "        for chunk in r.iter_content(1000000):\n",
    "            file.write(chunk)\n",
    "\n",
    "    # Parsing html for PubMed Trending Data\n",
    "    with open(f\"PubMed Trending Page{page}.html\", encoding='utf-8') as data:\n",
    "        dataSoup = bs4.BeautifulSoup(data, 'html.parser')\n",
    "        all_elems = dataSoup.find_all(class_='docsum-content')\n",
    "        contents = []\n",
    "\n",
    "        for items in all_elems:\n",
    "            id_match = re.findall(id_pattern, str(items))\n",
    "            years = re.findall(year_pattern, items.text.lstrip().rstrip('.'))\n",
    "            names = re.findall(name_pattern, items.text.lstrip().rstrip())\n",
    "            pmid = re.findall(pmid_pattern, items.text.lstrip().rstrip())\n",
    "            journals = re.findall(jour_pattern, items.text.lstrip().rstrip())\n",
    "            pub = re.findall(pub_pattern, items.text.lstrip().rstrip())\n",
    "            access = re.findall(access_pattern, items.text.lstrip().rstrip())\n",
    "            for id in id_match:\n",
    "                num_id = [id[18:-1]]\n",
    "                contents = contents + num_id\n",
    "                if pmid:\n",
    "                    for pid in pmid:\n",
    "                        pid = [pid.replace('PMID: ', '')]\n",
    "                        contents = contents + pid\n",
    "                else:\n",
    "                    contents.append('')\n",
    "                if years:\n",
    "                    contents = contents + years\n",
    "                else:\n",
    "                    contents.append('')\n",
    "                if names:\n",
    "                    contents = contents + names\n",
    "                else:\n",
    "                    contents.append('')\n",
    "                if journals:\n",
    "                    for journal in journals:\n",
    "                        journal = journal.replace('.\\nPMID:', '')\n",
    "                        jour = journal.split('.')\n",
    "                        jour.pop()\n",
    "                        contents = contents + jour\n",
    "                else:\n",
    "                    contents.append('')\n",
    "                if access:\n",
    "                    contents = contents + access\n",
    "                else:\n",
    "                    contents.append('')\n",
    "                if pub:\n",
    "                    contents = contents + pub\n",
    "                else:\n",
    "                    contents.append('')\n",
    "        for content in contents:\n",
    "            all_contents.append(content.replace(\"\\n\", '').lstrip('. ').rstrip('.'))\n",
    "trend_format = [all_contents[x:x + 7] for x in range(0, len(all_contents), 7)]\n",
    "trend_cols = ['ArticleID', 'PMID', 'Year', 'Title', 'Journal', 'Access', 'Details']\n",
    "\n",
    "# Writing PubMed Trending Data to CSV file.\n",
    "with open(\"PubMed Trending Data(Analysis).csv\", 'w', newline='', encoding='utf-8') as trending_csv:\n",
    "    write = csv.writer(trending_csv)\n",
    "    write.writerow(trend_cols)\n",
    "    write.writerows(trend_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web Scrapping PubMed Journal List \n",
    "r = requests.get(f\"https://ftp.ncbi.nih.gov/pubmed/J_Medline.txt\", allow_redirects=True)\n",
    "try:\n",
    "    r.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('There was a problem: %s' % (exc))\n",
    "\n",
    "# Creating text file\n",
    "with open(f\"PubMed Journal List.txt\", 'wb') as ref_file:\n",
    "    for chunk in r.iter_content(1000000):\n",
    "        ref_file.write(chunk)\n",
    "\n",
    "# Parsing Journal List text file\n",
    "with open(f\"PubMed Journal List.txt\", 'r') as jour_file:\n",
    "    journals = jour_file.readlines()\n",
    "    pattern = re.compile(r':.*')\n",
    "    matches = []\n",
    "    for lines in journals:\n",
    "        lines = lines.rstrip()\n",
    "        match = re.findall(pattern, lines)\n",
    "        for value in match:\n",
    "            if match:\n",
    "                matches.append(value.replace(':', '').lstrip())\n",
    "            else:\n",
    "                matches.append('')\n",
    "    data_format = [matches[x:x + 7] for x in range(0, len(matches), 7)]\n",
    "journal_cols = ['JrId', 'JournalTitle', 'MedAbbr', 'ISSN(Print)', 'ISSN(Online)', 'IsoAbbr', 'NlmId']\n",
    "\n",
    "# Writing Journal List Data to CSV file.\n",
    "with open(\"PubMed Journal Data(Analysis).csv\", 'w', newline='', encoding='utf-8') as journal_csv:\n",
    "    write = csv.writer(journal_csv)\n",
    "    write.writerow(journal_cols)\n",
    "    write.writerows(data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading PubMed Trending Data to pandas dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading PubMed Journal Data to pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to MS SQL server\n",
    "def connection(driver, server):\n",
    "    connection = pyodbc.connect(driver=driver, server=server               \n",
    "               trusted_connection='yes')\n",
    "    return connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to MS SQL and database\n",
    "def engine(driver, server,database):\n",
    "    cnxn = urllib.parse.quote_plus(f\"driver={driver}; server={server};database={database};trusted_connection=yes\")\n",
    "    engine = create_engine('mssql+pyodbc:///?odbc_connect={}'.format(cnxn))\n",
    "    return engine\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting PubMed Trending df to SQL\n",
    "df.to_sql('PubMed_Trending',con=engine,schema='dbo',if_exists='replace',index=True, index_label='PositionID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting PubMed Journal df to SQL\n",
    "df.to_sql('PubMed_Journal',con=engine,schema='dbo',if_exists='replace',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
